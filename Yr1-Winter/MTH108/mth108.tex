\documentclass[a4paper]{article}
\input{pre}

\title{MTH108 | Linear Algebra}
\author{James Li | 501022159 \and Professor: K. Q. Lan\and Email: klan@torontomu.ca}
\date{}
\renewcommand{\contentsname}{\centering Table of Contents}
\begin{document}
  \maketitle
  \tableofcontents
  \newpage
  \section{Euclidean Spaces} 
  A Euclidean Space is a mathematical space in which points and lines can be represented by a set of coordinates in the respective dimension of the space, and every point can be represented in a defined set. For example:
  $$
  \R^3 = {(x,y,z); x,y,z \in \R}
  $$
  is three-dimensional space represented using coordinates in terms of $(x,y,z)$, where $x,y,z \in \R$.
  \begin{theorem}
    Given two vectors $\vec{a},\vec{b}$ and some constant $k$, $\vec{a}$ and $\vec{b}$ are called \textbf{parallel} if:
    \begin{displaymath}
      \vec{a} = k\vec{b} \Leftrightarrow \vec{a} // \vec{b}
    \end{displaymath}
    
  \end{theorem}
  
  \subsection{Products of Vectors with Constants}
  \begin{theorem}
    Given a constant $k$ in $\R$ and some vector $\vec{a}$ in $\R^2$ , the product of $k \vec{a}$ is:
    \begin{displaymath}
      k \vec{a} = k(x_1,y_1) = (k\cdot x_1, k\cdot y_1), x,y \in \R
    \end{displaymath}
  \end{theorem}
  To represent a vector in Linear Algebra, we can use the following notation (\textit{using the previously mentioned vector $\vec{a}$ as an example}):
  $$
  \vec{a} = (x_1,y_1) = 
  \begin{pmatrix}
   x_1 \\ y_1 
  \end{pmatrix}
  $$
  \subsubsection{Examples:}
  Take $\vec{a} = (\begin{smallmatrix} 2 \\ -1\end{smallmatrix})$ and $\vec{b} = (\begin{smallmatrix} 4 \\ -1\end{smallmatrix})$, compute $-2\vec{a} + 3\vec{b}$:
  \begin{equation}
    \label{eq:1}
    \begin{split}
      -2\vec{a} + 3\vec{b} &= 2\begin{pmatrix}
       2 \\ 1 
      \end{pmatrix} + 3 \begin{pmatrix}
       4 \\ -1 
      \end{pmatrix}\\
                           &= \begin{pmatrix}
                           -4 \\ 2 
                           \end{pmatrix} + \begin{pmatrix}
                           12 \\ -3 
                           \end{pmatrix} \\
                           &= \begin{pmatrix}
                           8 \\ -1 
                           \end{pmatrix}
    \end{split}
  \end{equation}
  \subsection{Products of two Vectors}
  \begin{theorem}
    Given two vectors in $\R^n ,\vec{a} = (\begin{smallmatrix}x_1 \\ \vdots \\ x_n\end{smallmatrix}), \vec{b} = (\begin{smallmatrix}y_1\\ \vdots \\ y_n\end{smallmatrix})$, the product of $\vec{a} \cdot \vec{b}$ is:
    \begin{displaymath}
      \begin{split}
        \vec{a} \cdot \vec{b} &= \begin{pmatrix}
          x_1 & \dots & x_n 
        \end{pmatrix} \cdot \begin{pmatrix}
          y_1 \\ \vdots \\ y_n
      \end{pmatrix}\\
                              &= x_1y_1 + x_2y_2 + \dots + x_ny_n
      \end{split}
    \end{displaymath}
   This is known as the \textbf{Dot Product}.
  \end{theorem}
  \subsubsection{Examples:}
  Take $\vec{A} = (\begin{smallmatrix} 1 \\ -1 \\ 2 \\ 3\end{smallmatrix})$ and $\vec{b} = (\begin{smallmatrix}2 \\ 1 \\ -1 \\ 1\end{smallmatrix})$, Find the dot product of $\vec{a} \cdot \vec{b}$:
  \begin{equation}
    \label{eq:2}
    \begin{split}
      \vec{a} \cdot \vec{b} &= (2) + (-1) + (-2) + (3) \\
                            &= 2
    \end{split}
  \end{equation}
  \subsection{Other Properties of Vector Products}
  Given some $\vec{a},\vec{b}$ in $\R^n$ and some constant $k$, the following properties apply:
  \begin{itemize}
    \item $(\vec{a}+\vec{b})(\vec{a}+\vec{b}) = \vec{a}^2+2\vec{a}\vec{b}+\vec{b}^2$
    \item $\vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a}$
    \item $\vec{a} \cdot (\vec{b}+\vec{c}) = \vec{a}\vec{b} + \vec{a}\vec{c}$
    \item $\vec{a}(k\vec{b}) = k\vec{a}\vec{b}$
    \item $\vec{a} \cdot \vec{a} = \vec{a}^2 = x_1^2 + x_2^2 + \dots + x_n^2$
  \end{itemize}
  The midpoint $C(z_1,z_2,\dots,z_n)$ of a line from $A(x_1,x_2,\dots,x_n)$ to $B(y_1,y_2,\dots,y_n)$ is calculated using the following forumla:
  \[
    \begin{pmatrix}
    z_1 \\ z_2 \\ \vdots \\ z_n
    \end{pmatrix}
    = (1-t)
    \begin{pmatrix}
     x_1 \\ x_2 \\ \vdots \\ x_n 
    \end{pmatrix}
    + t
    \begin{pmatrix}
     y_1 \\ y_2 \\ \vdots \\ y_n 
    \end{pmatrix}
    \quad
    \textrm{for t such that }
    0 \le t \le 1
  \]
  We can simplify this to calculate every $z_i$:
  \[
    z_i = \displaystyle\frac{x_i+y_i}{2}
  \]
  \subsubsection{Examples:}
  Given $A(1,-2)$ and $B(-3,4)$, find the midpoint $C(x,y)$:
  \begin{equation}
    \begin{split}
      x &= \displaystyle\frac{1+(-3 )}{2} \\
        &= -1\\
      y &= \displaystyle\frac{(-2)+4 }{2} \\
        &= 1
    \end{split}
  \end{equation}
  Therefore $C = (-1,1)$.
  \subsection{Norm and Angle}
  The magnitude of a vector in $\R^n$ is called the \textbf{Norm}, it is notated and defined as:
  \[
    ||\vec{a}|| = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2} \quad (\vec{a}\textrm{ is some vector in }\R^n)
  \]
  We can use this definition to demonstrate some inequalities and properties (\textit{Given some $\vec{a},\vec{b} \in \R^n$ and some constant $k$}):
  \begin{itemize}
    \item $|| \vec{a} + \vec{b} ||  \le ||\vec{a}|| + ||\vec{b}||$ \textit{(Triangle Inequality)}
    \item $|| \vec{a} + \vec{b} || \geq ||\vec{a}|| - ||\vec{b}||$
    \item $|| k \vec{a} || = |k| \cdot || \vec{a}||$
    \item $|| \displaystyle\frac{\vec{a}}{||\vec{a}||}|| = 1$
    \item $\vec{a} \cdot \vec{b} = 0 \Leftrightarrow \vec{a} \bot \vec{b}$ \textit{(Orthagonal)}
    \item $\cos \theta = \displaystyle\frac{\vec{a}\cdot\vec{b}}{||\vec{a}||  ||\vec{b}||}$
  \end{itemize}
  \subsection{Determinants}
  The determinant of a matrix is a number that can be calculated using the following formula (\textit{in $\R^2$}):
  \[
    \begin{vmatrix}
      a & b \\ c & d
    \end{vmatrix}
    = ad - bc
  \]
  Given $\vec{a},\vec{b} \in \R^n$, the \textbf{Gram Determinant} of $\vec{a}\vec{b}$ is defined as:
  \[
    G(\vec{a},\vec{b})
    = \begin{vmatrix}
      \vec{a} \cdot \vec{a} & \vec{a} \cdot \vec{b} \\
      \vec{b} \cdot \vec{a} & \vec{b} \cdot \vec{b}
    \end{vmatrix}
    = ||\vec{a}||^2 \ ||\vec{b}||^2 - (\vec{a}\vec{b})^2
  \]
  Given some vectors $\vec{a},\vec{b} \in \R^3$, the Gram Determinant can be calculated using the following formula:
  \[
    G(\vec{a},\vec{b}) = \begin{vmatrix}
      x_1 & x_2 \\ y_1 & y_2 
    \end{vmatrix}^2 +
    \begin{vmatrix}
      x_1 & x_3 \\ y_1 & y_3
    \end{vmatrix}^2 +
    \begin{vmatrix}
      x_2 & x_3 \\ y_2 & y_3
    \end{vmatrix}^2
  \]
  \begin{lemma}
    The \textbf{Cauchy Inequality} states:
    \[
      G (\vec{a},\vec{b}) \geq 0 \Rightarrow |\vec{a} \cdot \vec{b}| \le || \vec{a}|| \cdot || \vec{b} ||
    \]
  \end{lemma}

  \section{Projections, Linear Combinations and Span} 
  For some $\vec{a},\vec{b} \in \R^n$, a projection of $\vec{a}$ on $\vec{b}$ is defined as:
  \[
    \vectorproj[\vec{a}]{\vec{b}} = \frac{\vec{a}\cdot\vec{b}}{||\vec{a}||^2} \cdot \vec{a}
  \]
  The norm of a projection is defined as:
  \[
    ||\vectorproj[\vec{a}]{\vec{b}}|| = \frac{\vec{a}\cdot\vec{b}}{||\vec{a}||^2}\cdot ||\vec{a}||
  \]
  \subsection{Projection and Area of a parallelogram}
  \begin{figure}[h]
    \begin{center}
      \includegraphics[width=0.5\textwidth]{img/2023-01-23-20-16-23.png}
    \end{center}
  \end{figure}
  
  Given the above figure, we can determine formulas for calculating properties of the parallelogram:

  \begin{enumerate}
    \item $h = ||\vec{BC}|| = ||\vec{b} - \vectorproj[\vec{a}]{\vec{b}}||$
    \item $Area = ||\vec{a}|| \cdot ||\vec{b} - \vectorproj[\vec{a}]{\vec{b}}||$
    \item $\sin(\theta) = \frac{||\vec{b}-\vectorproj[\vec{a}]{\vec{b}}||}{||\vec{b}||}$
    \item $\cos(\theta) = \frac{||\vectorproj[\vec{a}]{\vec{b}}||}{||\vec{b}||} = \frac{\vec{a}\cdot\vec{b}}{||\vec{a}||||\vec{b}||}$
  \end{enumerate}
  The generalized formula for calculating the area of some $\vec{a},\vec{b} \in \R^n$ is as follows:
  \[
    A(\vec{a},\vec{b}) = \sqrt{G(\vec{a},\vec{b})}
  \]
  \subsection{Linear Combinations and Span}
  Take some vectors $\vec{a},\vec{b} \in \R^n$, the linear combination($A$) of these vectors is simply:
  \[
    m \vec{a} + n\vec{b} = A\qquad n,m \in \R
  \]
  Span can be thought of as the set of values that you can reach by changing the constant in a linear combination of vectors, taking $\vec{a} = (1,2)$ and $\vec{b} = (2,3)$ the span would then be:
  \[
    span(a,b) = \R^2
  \]
  This is because you could theoretically reach any point in $\R^2$ by changing values of $m,n \in \R$, the exception to this is when both vectors are zero-vectors or when both vectors "align", in that case the set of values possible would only be the values extending the single line \textit{(e.g $\vec{a} = (1,0),\vec{b}=(-1,0)$) }.

  \section{Matrices} 
  A \textbf{matrix} $A$,  of $n \times m$ is defined as a rectangular array of $mn$ numbers arranged in $m,n$ rows and columns:
  \[
    A = \begin{pmatrix}
      a_{1,1} & a_{1,2} & a_{1,3}& \dots & a_{1,n} \\
      a_{2,1} & a_{2,2} & a_{2,3}&\dots & a_{2,n} \\
      a_{3,1} & a_{3,2} & a_{3,3}&\dots & a_{3,n} \\
      \vdots & \vdots & \vdots&\ddots & \vdots\\
      a_{m,1} & a_{m,2} & a_{m,3}&\dots & a_{m,n} \\
    \end{pmatrix}
  \]
  If we take only either columns or rows we can define:
  \[
    a_1 = \begin{pmatrix}
      a_{1,1}  & a_{1,2} & \dots & a_{1,n}
    \end{pmatrix} \textrm{ This is called a \textbf{Row Matrix}}
  \]
  \[
    a_1 = \begin{pmatrix}
      a_{1,1}  \\ a_{2,1} \\ \dots \\ a_{m,1}
    \end{pmatrix} \textrm{ This is called a \textbf{Column Matrix}}
  \]
  \subsection{Properties of Matrices}
  The \textbf{size} of a matrix is simply its number of rows $\times$ its number of columns:
  \[
    size\begin{pmatrix}
      1 & 2 & 3 \\ 4 & 5 & 6 
    \end{pmatrix} = 2\times 3 = 6
  \]
  The \textbf{transpose} of a matrix is defined as the matrix itself but with the rows and columns swapped, it is denoted with $^T$, take the previously defined matrix $A$, the transpose of $A = A^T$, would be denoted as:
  \[
    A^T = \begin{pmatrix}
      a_{1,1} & a_{2,1} & a_{3,1}& \dots & a_{m,1} \\
      a_{1,2} & a_{2,2} & a_{3,2}&\dots & a_{m,2} \\
      a_{1,3} & a_{2,3} & a_{3,3}&\dots & a_{m,3} \\
      \vdots & \vdots & \vdots&\ddots & \vdots\\
      a_{1,n} & a_{2,n} & a_{3,n}&\dots & a_{m,n} \\
    \end{pmatrix}
  \]
  Two matrices $A,B$ are said to be \textbf{equal}, if their size are the same and the respective entries are equal:
  \[
    A = \begin{pmatrix}
      1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 
    \end{pmatrix} =
    \begin{pmatrix}
      1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 
    \end{pmatrix} = B
  \]
  Matrix \textbf{addition, subtraction and scalar multiplication} can all be achieved in the same method as their vector counterparts.
  \subsection{Product of a Vector and Matrix}
  The product of matrix $A$ with a row/column vector $X$ is a vector where the values are each entry in the row/column of $A$ multiplied by the $n$th element in $X$ for that row/column, then summed. For example, take $X = (x_1,x_2,x_3,\dots,x_n)$
  \[
    AX = \begin{pmatrix}
      a_{1,1}x_{1} &+& a_{1,2}x_{1} &+& a_{1,3}x_{1} &+& \dots &+& a_{1,n}x_{1}\\
      a_{2,1}x_{2} &+& a_{2,2}x_{2} &+& a_{2,3}x_{2} &+& \dots &+& a_{1,n}x_{2}\\
      a_{3,1}x_{3} &+& a_{3,2}x_{3} &+& a_{3,3}x_{3} &+& \dots &+& a_{1,n}x_{3}\\
      \vdots & \ & \vdots & \ & \vdots & \ & \vdots & \ & \vdots \\
      a_{m,1}x_{n} &+& a_{m,2}x_{n} &+& a_{m,3}x_{n} &+& \dots &+& a_{m,n}x_{n}\\
    \end{pmatrix}
  \]
  If $X = (\begin{smallmatrix} x_1 \\ x_2 \\ \dots \\ x_n  \end{smallmatrix})$ instead, then $AX$ would be:
  \[
    AX = \begin{pmatrix}
      a_{1,1}x_{1} &+& a_{1,2}x_{2} &+& a_{1,3}x_{3} &+& \dots &+& a_{1,n}x_{n}\\
      a_{2,1}x_{1} &+& a_{2,2}x_{2} &+& a_{2,3}x_{3} &+& \dots &+& a_{1,n}x_{n}\\
      a_{3,1}x_{1} &+& a_{3,2}x_{2} &+& a_{3,3}x_{3} &+& \dots &+& a_{1,n}x_{n}\\
      \vdots & \ & \vdots & \ & \vdots & \ & \vdots & \ & \vdots \\
      a_{m,1}x_{1} &+& a_{m,2}x_{2} &+& a_{m,3}x_{3} &+& \dots &+& a_{m,n}x_{n}\\
    \end{pmatrix}
  \]
  Note that a matrix and vector can only be multiplied if the number of columns of the matrix is equal to the number of elements in the vector. This can also be called the \textbf{image} of $X$ under $A$.
  \subsection{Product of two Matrices}
  Given matrcies $A,B$, their product $AB$, can be calculated by taking the \textbf{dot product} between the first row of $A$ with the first column of $B$, then the first row of $A$ with the second column of $B$, until the $n$th column of $B$, then you repeat the process until you reach the $n$th row of $A$. Take the following example:
  \[
    A = \begin{pmatrix}
      1 & 2 \\ 3 & 4 
    \end{pmatrix}
    B = \begin{pmatrix}
      5 & 6 \\ 7 & 8 
    \end{pmatrix}
  \]
  \[
    \begin{split}
      AB = &((1,2) \cdot (5,7) + (1,2) \cdot (6,8))\\
         & ((3,4)\cdot (5,7) + (3,4) \cdot (6,8))\\
         &= \begin{pmatrix}
           19 & 22 \\ 43 & 50
         \end{pmatrix}
    \end{split}
  \]
  Two matrices $A,B$ with sizes $size(A) = n \times m$ and $size(B) = i \times j$, with $n,m,i,j \in \R$, $AB$ can only be evaluated if $m = i$. The size of the resulting matrix $AB$ will be $n\times j$. Keep in mind that commutativity does apply in regard to matrix multiplication, hence $AB \not = BA$ may be true. Also keep in mind the following formula:

  \[
    (AB)^T = B^T A^T
  \]

  \subsection{Square Matrices}\label{squaremat}
  A square matrix is a matrix of $n \times n$ dimensions, for example:
  \[
    A = \begin{pmatrix}
      a_{1,1} & a_{1,2} & a_{1,3}& \dots & a_{1,n} \\
      a_{2,1} & a_{2,2} & a_{2,3}&\dots & a_{2,n} \\
      a_{3,1} & a_{3,2} & a_{3,3}&\dots & a_{3,n} \\
      \vdots & \vdots & \vdots&\ddots & \vdots\\
      a_{n,1} & a_{n,2} & a_{n,3}&\dots & a_{n,n} \\
    \end{pmatrix}
  \]
  The \textbf{trace} of a square matrix is defined as the sum of all the diagonal elements:
  \[
    tr(A) = a_{1,1} + a_{2,2} + \dots + a{n,n}
  \]
  If all the elemnts above/below the diagonal line in a matrix is 0, then the matrix is called a \textbf{lower/upper} triangle matrix respectively. If all values on either side of the diagonal are 0, then it is called a \textbf{upper and lower} triangle matrix.
  \newpage
  Let $A$ be a $n\times n $ matrix, the following properties then apply:
  \begin{itemize}
    \item If $A^T = A$ then $A$ is called \textbf{symmetric}.
    \item $A^n = \Pi^n_0 A$ 
    \item $A^0 = I$ where $I$ is an identity matrix with the same dimensions as $A$.
  \end{itemize}
  We define a diagonal matrix in the form $diag(a_{1,1},a_{2,2},\dots,a_{n,n})$ as follows:
  \[
    A = \begin{pmatrix}
      a_{1,1} & 0 & 0& \dots & 0 \\
      0 & a_{2,2} & 0&\dots & 0 \\
      0 & 0 & a_{3,3}&\dots & 0 \\
      \vdots & \vdots & \vdots&\ddots & \vdots\\
      0 & 0 & 0&\dots & a_{n,n} \\
    \end{pmatrix}
  \]
  Given some identity matrix $I$ with the same dimensions as the previous diagonal matrix, then the following property holds:
  \[
    IA = AI
  \]
  For a diagonal matrix, the following property also applies for some $k\in \R$:
  \[
    A = \begin{pmatrix}
      a_{1,1} & 0 & 0& \dots & 0 \\
      0 & a_{2,2} & 0&\dots & 0 \\
      0 & 0 & a_{3,3}&\dots & 0 \\
      \vdots & \vdots & \vdots&\ddots & \vdots\\
      0 & 0 & 0&\dots & a_{n,n} \\
    \end{pmatrix}^k = 
    A = \begin{pmatrix}
      a_{1,1}^k & 0 & 0& \dots & 0 \\
      0 & a_{2,2}^k & 0&\dots & 0 \\
      0 & 0 & a_{3,3}^k&\dots & 0 \\
      \vdots & \vdots & \vdots&\ddots & \vdots\\
      0 & 0 & 0&\dots & a_{n,n}^k \\
    \end{pmatrix}
  \]
  Keep in mind this \textbf{only} applies to diagonal matrices.
  \section{Row Echleon Form \& Reduced Row Echleon Form} 
  \subsection{Elementary Row Operations}
  There are 3 operations which can be performed on any matrix, with the result being considered \textbf{row equivalent} to the original matrix, meaning that they represent the same matrix (\textit{Think about how $\frac{4 }{3} = \frac{8 }{6}$}). From now on, the $n$th row of a matrix will be referred to as $Rn$, the operations are as follows:
  \begin{enumerate}
    \item \underline{Interchange two rows}\newline
      \textbf{Example:} Interchange $R1$ with $R2$ in the following matrix.
      \[
        \begin{pmatrix}
          1 & 2 & 3 & 4 \\  
          5 & 6 & 7 & 8 \\  
          9 & 10 & 11 & 12 \\  
          13 & 14 & 15 & 16 \\  
        \end{pmatrix} \overset{R1 \leftrightarrow R2}{\Rightarrow}
        \begin{pmatrix}
          5 & 6 & 7 & 8 \\  
          1 & 2 & 3 & 4 \\  
          9 & 10 & 11 & 12 \\  
          13 & 14 & 15 & 16 \\  
        \end{pmatrix}
      \]
    \item \underline{Multiply all elements in a row by some constant $>$ 0}\newline
      \textbf{Example:} Multiply $R1$ by 3.
      \[
        \begin{pmatrix}
          1 & 2 & 3 & 4 \\  
          5 & 6 & 7 & 8 \\  
          9 & 10 & 11 & 12 \\  
          13 & 14 & 15 & 16 \\  
        \end{pmatrix} \overset{R1 = 3R1}{\Rightarrow}
        \begin{pmatrix}
          3 & 6 & 9 & 12 \\  
          5 & 6 & 7 & 8 \\  
          9 & 10 & 11 & 12 \\  
          13 & 14 & 15 & 16 \\  
        \end{pmatrix}
      \]
    \item \underline{Add or Subtract some nonzero multiple of one row to another}\newline
      \textbf{Example:} Add $3R2$ to $R1$.
      \[
        \begin{pmatrix}
          1 & 2 & 3 & 4 \\  
          5 & 6 & 7 & 8 \\  
          9 & 10 & 11 & 12 \\  
          13 & 14 & 15 & 16 \\  
        \end{pmatrix} \overset{R1 = R1 + 3R2}{\Rightarrow}
        \begin{pmatrix}
          16 & 20 & 24 & 28 \\  
          5 & 6 & 7 & 8 \\  
          9 & 10 & 11 & 12 \\  
          13 & 14 & 15 & 16 \\  
        \end{pmatrix}
      \]
  \end{enumerate}
  \subsection{Row Echleon Form (REF)}
  For any matrix to be in row echleon form (REF), it must satisfy the following conditions:
  \begin{itemize}
    \item Every row of nonzero elements must be \underline{above} rows of all zero.
    \item Each leading nonzero element of a row must be in a column to the \underline{right} of the leading element in the row above it.
    \item Every element in the column \underline{below} a leading element must be zero.
  \end{itemize}
  Note however, every element in the column \underline{above} a leading element does not have to be zero. The first nonzero element in a row is called the \textbf{pivot} and every column with a pivot element is called a \textbf{pivot column}. The following is a matrix in REF:
  \[
    \begin{pmatrix}
      \fbox{1} & 2 & 0 & 1 \\  
      0 & \fbox{2} & 3 & 1 \\  
      0 & 0 & \fbox{4} & 0 \\  
      0 & 0 & 0 & 0 \\  
    \end{pmatrix}
  \]
  Where the elements encased with \fbox{\textit{Element}} are the leading elements of that row.
  \subsection{Reduced Row Echleon Form (RREF)}\label{rref}
  For any matrix to be in reduced row echleon form (RREF), it must satisfy the following conditions:
  \begin{itemize}
    \item The matrix is already in REF.
    \item The leading nonzero entry in each row \textbf{must} be equal to 1.
    \item Each leading 1 is the \textbf{only} nonzero element in the entire column.
  \end{itemize}
  If we performed row operations on the matrix from REF definition, it would look as follows in RREF:
  \[
    \begin{pmatrix}
      1 & 0 & 0 & 0 \\  
      0 & 1 & 0 & \frac{1 }{2} \\  
      0 & 0 & 1 & 0 \\  
      0 & 0 & 0 & 0 \\  
    \end{pmatrix}
  \]
  \subsection{Ranks and Nullities}
  \begin{itemize}
    \item The \textbf{rank} of some matrix $A$ is the number of pivots it contains.
    \item The \textbf{nullity} of some matrix $A$ is the number of columns $n$ minus the rank.
  \begin{theorem}
    For any matrix $A$, the following statements hold,
    \begin{enumerate}
      \item $r(A) = r(A^t)$
      \item $r(A) \le min\{m,n\}$, where $m,n$ are dimensions of $A$.
    \end{enumerate}
  \end{theorem}
  
  \end{itemize}
  \newpage
  \section{The Inverse of a Square Matrix} \label{sqmat}
  Given a matrix $A_{n\times n}$, the inverse of A; $\frac{1 }{A_{n,n}}$ is defined as:
  \begin{defn}
    If there exists $B_{n\times n}$, such that $AB=I$, then $A$ is \textbf{invertible} and:
    \[
      A^{-1}=B\rightarrow AA^{-1} = I
    \]
  \end{defn}
  \noindent
  For some $2\times 2$ matrix $A$; $A$ is invertible if $|A| = |\begin{smallmatrix} a & b \\ c & d \end{smallmatrix}| = ad - bc \neq 0$. The following properties apply for all $2\times 2$ matrcies.
  \begin{enumerate}
    \item $|A| = 0 \rightarrow A^{-1} = \frac{1 }{|A|} \begin{pmatrix}
        d & -b \\
        -c & a
    \end{pmatrix}$
  \item $r(A) = 2 \rightarrow$ $A$ is invertible.
  \item $r(A) \le 1 \rightarrow$ $A$ is not invertible.
  \end{enumerate}
  For all matrices of $n \times n$, $A$, the following apply:
  \begin{enumerate}
    \item $r(A) = n \rightarrow A$ is invertible.
    \item $|A| \neq 0 \rightarrow A$ is invertible.
    \item $(AB)^{-1} = B^{-1}A^{-1}$
    \item $(kA)^{-1} = k^{-1}A^{-1}, k \neq 0$ for some constant $k$.
  \end{enumerate}
  If $A$ is determined to be invertible, take the following steps to find $A^{-1}$:
  \[
    (A|I) \rightarrow (B|C)_{REF} \rightarrow (I|A)_{RREF}
  \]
  \section{Determinants} 
  \subsection{The Minor and Cofactor}
  We start by defining the \textbf{minor} of the matrix as the determinant for when the $i,j$th row and column are removed. This is notated as $M_{ij}$ of some matrix $A$, for example:
  \[
    M_{12} \textrm{ of }
    \begin{pmatrix}
      1 & 2 & 3 \\ 
      4 & 5 & 6 \\ 
      7 & 8 & 9 \\ 
    \end{pmatrix}
    =
    \begin{pmatrix}
      4 & 6 \\ 
      7 & 9 \\ 
    \end{pmatrix}
    = \underset{ad}{(36)} - \underset{bc}{(42)} = -6
  \]
  A \textbf{matrix of minors} is simply a matrix of equal size but with each corresponding element index replaced with the minor of the same index, for some matrix $A$:
  \[
    \textrm{Matrix of minors of }
    \begin{pmatrix}
      a_{1,1} & a_{1,2} & a_{1,3}& \dots & a_{1,n} \\
      a_{2,1} & a_{2,2} & a_{2,3}&\dots & a_{2,n} \\
      a_{3,1} & a_{3,2} & a_{3,3}&\dots & a_{3,n} \\
      \vdots & \vdots & \vdots&\ddots & \vdots\\
      a_{n,1} & a_{n,2} & a_{n,3}&\dots & a_{n,n} \\
    \end{pmatrix}
    = 
    \begin{pmatrix}
      M_{1,1} & M_{1,2} & M_{1,3}&\dots & M_{1,n} \\
      M_{2,1} & M_{2,2} & M_{2,3}&\dots & M_{2,n} \\
      M_{3,1} & M_{3,2} & M_{3,3}&\dots & M_{3,n} \\
      \vdots & \vdots & \vdots&\ddots & \vdots\\
      M_{n,1} & M_{n,2} & M_{n,3}&\dots & M_{n,n} \\
    \end{pmatrix}
  \]
  The \textbf{cofactor} of any element in a matrix $A$, is either equal to the minor of that element or the opposite of the minor, depending on where the element is located. If the row and column of the element add up to be an \textbf{even} number, then the cofactor is the same as the minor, otherwise, if it is an \textbf{odd} number it is equal to the opposite of the minor (inverse the sign).
  The \textbf{matrix of cofactors} of a matrix $A$ can be found by finding the matrix of minors and applying the appropriate sign changes.
  \subsection{The Determinant of a \textit{n} x \textit{n} Matrix}
  To find the determinant of a matrix $A_{n\times n}$, apply these operations:
  \begin{enumerate}
    \item Choose any row or column in the matrix, there is no criteria for which you choose, for efficiency sake you should choose the one which looks the simplest.
    \item Multiply every element in the row or column by its \textbf{cofactor} then sum, the result will be the determinant.
  \end{enumerate}
  For example, find the determinant of the following matrix by expanding the first row:
  \[
    det(
    \begin{pmatrix}
      3 & 0 & 1 \\
      4 & 2 & 2 \\
      7 & 2 & 5 \\
    \end{pmatrix}
    )=
    3 \begin{vmatrix}
      2 & 2 \\ 
      2 & 5 \\ 
    \end{vmatrix} -
    0 \begin{vmatrix}
      3 & 1 \\ 
      7 & 5 \\ 
    \end{vmatrix} + 
    1 \begin{vmatrix}
      4 & 2 \\ 
      7 & 2 \\ 
    \end{vmatrix} = 3(6) - 0 + 1(-6) = 12
  \]
  This applies for any value of $n$, the bigger the matrix the more times you would have to repeat this operation. The determinant of a matrix will be \textbf{zero} if any of the following properties hold:
  \begin{itemize}
    \item An entire row consists of only 0.
    \item Two rows or columns are equal.
    \item A row or column is equal to a multiple of another row or column.
  \end{itemize}
  \subsection{Evaluating Determinants by Row Operation}
  To simplify the process of finding determinants we can use row operations to convert some matrix $A$ into an \textbf{upper triangular matrix}(\textit{see} \ref{squaremat}), where then the determinant is simply $|A| = k|B|, k\in \R$ and $B$ is the upper triangular matrix of $A$.
  We know the following properties of determinants:
  \begin{itemize}
    \item $A \overset{\displaystyle{R_i (\frac{1 }{c})}}{\longrightarrow} B$, then $|A| = c|B|$
    \item $A \overset{\displaystyle{R_{ij}}}{\longrightarrow} B$, then $|A| = -|B|$
    \item $A \overset{\displaystyle{R_i(c)+R_j }}{\longrightarrow} B$, then $|A| = |B|$
  \end{itemize}
  After finding the upper triangle matrix, evaluating the determinant should be straight forward, as you can simply take the expansion of the bottom row, which will be all 0 except for the final element, meaning you would only have to perform expansion operations using the final element.
  \subsection{Properties of Determinants}
  Given some matrix $A \in \R^{n\times n}$, the following properties hold:
  \begin{itemize}
    \item $A$ is invertible $\Leftrightarrow r(A) = n$ or $|A| \neq 0$
    \item $(A|I)$ can be changed into $(I|A^{-1})$
    \item $|AB| = |A||B| \rightarrow |AA^{-1}| = |A||A^{-1}| = 1$
    \item $|A^T| = |A|$
    \item $|kA| = k^n|A|$
  \end{itemize}
  \newpage
  \section{Systems of Linear Equations} 
  \subsection{Gaussian Elimination}\label{gausselim}
  Gaussian Elimination is a technique for solving systems of equations in the form:
  \[
    Ax = b 
  \]
  For some matrix $A \in \R^{n\times n}$ as the coefficient for some column vector $x \in \R^n$, where the dot product is some column vector $b \in \R^n$. Take the following steps to perform Gaussian Elimination:

  \begin{enumerate}
    \item Rewrite the system of equations as an augmented matrix with $(A|b)x$ 
      \[
        \begin{bmatrix}[ccccc|c]
          a_{1,1} & a_{1,2} & a_{1,3}& \dots & a_{1,n} & b_1\\
          a_{2,1} & a_{2,2} & a_{2,3}&\dots & a_{2,n} & b_2\\
          a_{3,1} & a_{3,2} & a_{3,3}&\dots & a_{3,n} & b_3\\
          \vdots & \vdots & \vdots&\ddots & \vdots& \vdots\\
          a_{n,1} & a_{n,2} & a_{n,3}&\dots & a_{n,n} & b_n\\
        \end{bmatrix}
        \begin{bmatrix}
         x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n 
        \end{bmatrix}
      \]
    \item Then using row operations, convert the augmented matrix $A|b$ into an \textbf{upper triangle matrix} (\textit{see} \ref{squaremat}) 
      \[
        \begin{bmatrix}[ccccc|c]
          a_{1,1}^\prime & a_{1,2}^\prime & a_{1,3}^\prime& \dots & a_{1,n}^\prime & b_1^\prime\\
          0 & a_{2,2}^\prime & a_{2,3}^\prime&\dots & a_{2,n}^\prime & b_2^\prime\\
          0 & 0 & a_{3,3}^\prime&\dots & a_{3,n}^\prime & b_3^\prime\\
          \vdots & \vdots & \vdots&\ddots & \vdots& \vdots\\
          0 & 0 & 0&\dots & a_{n,n}^\prime & b_n^\prime\\
        \end{bmatrix}
        \begin{bmatrix}
         x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n 
        \end{bmatrix}
      \]
    \item Then solve the equation of the $n$th row, for $x_n$, then substitute back into the $(n-1)$th row and solve recursively.
  \end{enumerate}
  \subsection{Gauss-Jordan Elimination}
  When using Gaussian Elimination to solve a system of equations, you will result with an augmented matrix $A|b$ where the matrix $A$ will be in row echleon form, if you simply continue reducing $A|b$ until $A$ is in reduced-row echleon form (\textit{see} \ref{rref}), where then solving for each $x_n$ will be trivial as there will only be one coefficient for each variable, this is called \textbf{Gauss-Jordan Elimination}.
  \begin{enumerate}
    \item Taking the augmented matrix from (\ref{gausselim}), we can continue reducing until reaching
      \[
        \begin{bmatrix}[ccccc|c]
          1 & 0 & 0& \dots & 0 & b_1^\prime\\
          0 & 1 & 0&\dots & 0 & b_2^\prime\\
          0 & 0 & 1&\dots & 0 & b_3^\prime\\
          \vdots & \vdots & \vdots&\ddots & \vdots& \vdots\\
          0 & 0 & 0&\dots & 1 & b_n^\prime\\
        \end{bmatrix}
        \begin{bmatrix}
         x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_n 
        \end{bmatrix}
      \]
    \item We can observe then that simply $x_n = b_n^\prime$
  \end{enumerate}
  \subsection{Inverse Matrix Method }
  Given a system of equations in the form $Ax=b$, we can utilize the \textbf{inverse} (\textit{see }\ref{sqmat}) of the matrix $A$ to find the solution set. We can derive the following:
  \begin{equation}
    \begin{aligned}
      Ax&=b \\
      (A^{-1})Ax &=(A^{-1})b\\
      (A^{-1}A)x &= (A^{-1})b\\
      Ix &= (A^{-1})b\\
      x &= (A^{-1})b
    \end{aligned}
  \end{equation}
  The solution set is then trivial to evaluate by taking the product of the inverse of the coefficient matrix with $b$. Note that if $A$ is not invertible $\neq$ there is no solution.
  \subsection{Cramer\textquotesingle s rule}
  Given a system of equations in the form $Ax=b$, \textbf{Cramer\textquotesingle s rule} states that we can take the column vector $b$, and substitute into each column of $A$, then find the determinant of the new matrix, the solution of each corresponding value in the variable column will be equal to the determinant of the matrix with the same number column substituted divided by the determinant of $A$.
  \begin{equation}
    \begin{aligned}
      Ax&=b \\
      \begin{pmatrix}
        1 & 1 & 2 \\ 
        2 & 1 & 3 \\ 
        1 & 2 & 4 \\ 
      \end{pmatrix}
      \begin{pmatrix}
       x_1 \\ x_2 \\ x_3 
     \end{pmatrix}&=
     \begin{pmatrix}
     2 \\ 1 \\ 3 
     \end{pmatrix}\\
      x_1 &= \displaystyle\frac{D_1}{D}\\
      x_2 &= \displaystyle\frac{D_2}{D}\\
      x_3 &= \displaystyle\frac{D_3}{D}\\
    \end{aligned}
  \end{equation}
  Where $D$ is the determinant of $A$ and $D_n$ is the determinant of $A$ with the $n$th column replaced by $b$.
  \subsection{Consistency of Systems of Linear Equations}
  Given a system of equations in the form $Ax = b$, we can determine the consistency of the system by observing the rank of the coefficient matrix $A$:
  \begin{itemize}
    \item $r(A) = r(A|b) = n \Rightarrow$ There exists a \textbf{unique} solution.
    \item $r(A) = r(A|b) < n \Rightarrow$ There exists \textbf{infinitely} many solutions.
    \item $r(A) < r(A|b) \Rightarrow$ There exists \textbf{no} solution.
  \end{itemize}
  \subsection{Linear Combination, Spanning Space and Consistency}
  \section{Linear Transformations} 
  Let $A$ be a $n\times m$ matrix, a \textbf{linear transformation} from $\R^n$ to $\R^m$ denoted $T$ is:
  \[
    T(\vec{x})= A\vec{x} = 
    \begin{pmatrix}
      a_{11} & \dots & a_{1n} \\
      \vdots & \ddots & \vdots \\
      a_{m1} & \dots & a_{mn}
    \end{pmatrix}
    \begin{pmatrix}
     x_1 \\
     \vdots \\
     x_n
    \end{pmatrix}
  \]
  We can also say that $T$ \textbf{maps} $\R^n$ to $\R^m$, denoted as:
  \[
    T: \R^n \rightarrow \R^m
  \]
  \subsection{One-to-one, Onto and Inverse Functions}
  \section{Placeholder} 
  \section{Placeholder} 
\end{document}
